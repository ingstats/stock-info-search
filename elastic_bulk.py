from elasticsearch import Elasticsearch
from elasticsearch import helpers
import pandas as pd
import json

# âœ… Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
es = Elasticsearch("http://localhost:9200", http_compress=True)
index_name = "stock_info"

# âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ë‹¤ì‹œ ìƒì„±
if es.indices.exists(index=index_name):
    es.options(ignore_status=[400, 404]).indices.delete(index=index_name)

# âœ… ìˆ˜ì •ëœ ë§¤í•‘ ì„¤ì • (`suggest` í•„ë“œë¥¼ ë³„ë„ í•„ë“œë¡œ ë¶„ë¦¬)
optimized_mapping = {
    "settings": {
        "analysis": {
            "tokenizer": {
                "edge_ngram_tokenizer": {
                    "type": "edge_ngram",
                    "min_gram": 2,
                    "max_gram": 10,
                    "token_chars": ["letter", "digit", "whitespace"]
                }
            },
            "analyzer": {
                "korean_analyzer": {
                    "type": "custom",
                    "tokenizer": "nori_tokenizer",
                    "filter": ["lowercase", "nori_readingform", "nori_number"]
                },
                "edge_ngram_analyzer": {
                    "tokenizer": "edge_ngram_tokenizer",
                    "filter": ["lowercase"]
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "company": {
                "type": "text",
                "analyzer": "edge_ngram_analyzer",
                "search_analyzer": "korean_analyzer",
                "fields": {
                    "keyword": {"type": "keyword"}
                }
            },
            "company_suggest": {
                "type": "completion"
            },
            "industry": {
                "type": "text",
                "analyzer": "edge_ngram_analyzer",
                "search_analyzer": "korean_analyzer",
                "fields": {
                    "keyword": {"type": "keyword"}
                }
            },
            "industry_suggest": {
                "type": "completion"
            },
            "founded_date": {"type": "date", "format": "yyyy-MM-dd"}
        }
    }
}

# âœ… ì¸ë±ìŠ¤ ìƒì„±
es.options(ignore_status=[400]).indices.create(index=index_name, body=optimized_mapping)

# âœ… KRX ë°ì´í„° í¬ë¡¤ë§ ë° ë³€í™˜
def get_stock_info():
    base_url = "http://kind.krx.co.kr/corpgeneral/corpList.do?method=download"
    df = pd.read_html(base_url, header=0, encoding='euc-kr')[0]

    # âœ… í•„ë“œëª… ë³€í™˜
    column_mapping = {
        'íšŒì‚¬ëª…': 'company',
        'ì—…ì¢…': 'industry',
        'ìƒì¥ì¼': 'founded_date'
    }

    # âœ… ì»¬ëŸ¼ëª… ë³€ê²½ í›„ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
    df = df.rename(columns=column_mapping)
    df = df[list(column_mapping.values())]

    # âœ… ê²°ì¸¡ê°’ ì œê±°
    df = df.dropna(subset=["company", "industry"])

    # âœ… NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜
    df = df.where(pd.notna(df), None)

    # âœ… ë‚ ì§œ ë³€í™˜ (ì„¤ë¦½ì¼)
    if "founded_date" in df.columns:
        df['founded_date'] = pd.to_datetime(df['founded_date'], errors='coerce').dt.strftime('%Y-%m-%d')

    return df

# âœ… ë°ì´í„° ì‚½ì… (ë°°ì¹˜ í¬ê¸°: 500)
df = get_stock_info()

if df.empty:
    print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    json_records = df.to_dict(orient='records')

    # âœ… `suggest` í•„ë“œ ì¶”ê°€ (ë³„ë„ í•„ë“œ ì‚¬ìš©)
    for record in json_records:
        if isinstance(record.get("company"), str):
            record["company_suggest"] = {"input": [record["company"]]}
        if isinstance(record.get("industry"), str):
            record["industry_suggest"] = {"input": [record["industry"]]}

    batch_size = 500
    failed_documents = []  # âŒ ì‹¤íŒ¨í•œ ë¬¸ì„œ ëª©ë¡ ì €ì¥

    for i in range(0, len(json_records), batch_size):
        batch = json_records[i:i + batch_size]
        action_list = [{"_op_type": "index", "_index": index_name, "_source": row} for row in batch]

        try:
            success, failed = helpers.bulk(es, action_list, stats_only=False, raise_on_error=False)
            print(f"âœ… {success}ê°œ ë°ì´í„° ì‚½ì… ì™„ë£Œ")

            # âŒ ì‹¤íŒ¨í•œ ë¬¸ì„œ í™•ì¸ ë° ì €ì¥
            if failed:
                print(f"âŒ {failed}ê°œ ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨! ìƒì„¸ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸ ì¤‘...")
                for doc, error in zip(batch, failed):
                    failed_documents.append({"document": doc, "error": error})
                    print(f"âŒ ì‚½ì… ì‹¤íŒ¨ ë¬¸ì„œ: {json.dumps(doc, ensure_ascii=False, indent=2)}")
                    print(f"ğŸ“Œ ì˜¤ë¥˜ ë‚´ìš©: {json.dumps(error, ensure_ascii=False, indent=2)}")

        except Exception as e:
            print(f"âŒ ë°ì´í„° ì‚½ì… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")

    # âœ… ì‹¤íŒ¨í•œ ë¬¸ì„œ ì €ì¥
    if failed_documents:
        with open("failed_documents.json", "w", encoding="utf-8") as f:
            json.dump(failed_documents, f, ensure_ascii=False, indent=4)
        print("ğŸ“ ì‹¤íŒ¨í•œ ë¬¸ì„œë¥¼ `failed_documents.json` íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
